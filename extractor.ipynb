{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335869cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports\n",
    "\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ollama client\n",
    "\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the client is responsive\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"Hi there!\"}]\n",
    "\n",
    "# model = \"llama3.1\"\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=model,\n",
    "#     messages=messages\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e15c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to stream response from the model, update the display in real-time, and return the final response as a string\n",
    "\n",
    "def stream_llm_response(model, messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    full_response = \"\"\n",
    "    display_id = None\n",
    "\n",
    "    for chunk in response:\n",
    "        content = chunk.choices[0].delta.content or \"\"\n",
    "        full_response += content\n",
    "\n",
    "        if display_id is None:\n",
    "            display_id = display(Markdown(full_response), display_id=True).display_id\n",
    "        else:\n",
    "            update_display(Markdown(full_response), display_id=display_id)\n",
    "\n",
    "    return full_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the streaming function with a simple prompt\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"Tell a funny joke for a Machine Learning engineer.\"}]\n",
    "# model = \"llama3.1\"\n",
    "# response = stream_llm_response(model=model, messages=messages)\n",
    "# print(\"\\n\\n--------------------------------------------------------------------------------\\n\\n\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the system prompt for the extraction task\n",
    "\n",
    "# system_prompt = \"\"\"\n",
    "# You are an AI assistant that extracts **accurate, structured insights** from markdown articles.\n",
    "\n",
    "# ### Task\n",
    "# From the given markdown article, produce:\n",
    "\n",
    "# ## Key Takeaways\n",
    "# - Core conclusions, claims, or findings\n",
    "# - Important facts, numbers, decisions, or timelines\n",
    "\n",
    "# ## Summary\n",
    "# - A concise but complete overview\n",
    "# - Capture the main topic, purpose, and key points\n",
    "\n",
    "# ### Rules\n",
    "# - Preserve meaning, context, and important qualifiers\n",
    "# - Do not add opinions or external information\n",
    "# - Do not include meta commentary\n",
    "# - Use clear markdown headings and bullet points\n",
    "# - Be concise but information-dense\n",
    "# \"\"\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert at extracting accurate, structured summaries and key takeaways from markdown articles.\n",
    "You are given a web scraped markdown article. \n",
    "Your task is to extract the key takeaways and a concise summary from the article.\n",
    "The output should be in markdown format.\n",
    "Reply only with the markdown content, do not include any explanations or commentary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the user prompt for the extraction task (for scraped-content/main_content.md)\n",
    "\n",
    "with open(\"scraped-content/main_content.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Here is a web scraped markdown article. \n",
    "Extract the key takeaways and a concise summary according to the system prompt instructions.\n",
    "\n",
    "Article:\n",
    "{content}\n",
    "\n",
    "Return the output in markdown format. \n",
    "Do not include any explanations or commentary, reply only with the markdown content.\n",
    "Start generating the response immediately.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dacf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1127c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model to perform the extraction task\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "model = \"llama3.1\"\n",
    "\n",
    "response = stream_llm_response(model=model, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843be6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "article-explorer (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
