{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335869cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adity\\OneDrive\\Desktop\\AI Projects\\article-explorer\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# all imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4836db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "652527b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# login to Hugging Face Hub using the token from environment variables\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c47c0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize the ollama client\n",
    "\n",
    "# OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "# client = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86b5d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if the client is responsive\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"Hi there!\"}]\n",
    "\n",
    "# model = \"llama3.1\"\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#     model=model,\n",
    "#     messages=messages\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e15c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # function to stream response from the model, update the display in real-time, and return the final response as a string\n",
    "\n",
    "# def stream_llm_response(model, messages):\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=messages,\n",
    "#         stream=True\n",
    "#     )\n",
    "\n",
    "#     full_response = \"\"\n",
    "#     display_id = None\n",
    "\n",
    "#     for chunk in response:\n",
    "#         content = chunk.choices[0].delta.content or \"\"\n",
    "#         full_response += content\n",
    "\n",
    "#         if display_id is None:\n",
    "#             display_id = display(Markdown(full_response), display_id=True).display_id\n",
    "#         else:\n",
    "#             update_display(Markdown(full_response), display_id=display_id)\n",
    "\n",
    "#     return full_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f8be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test the streaming function with a simple prompt\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": \"Tell a funny joke for a Machine Learning engineer.\"}]\n",
    "# model = \"llama3.1\"\n",
    "# response = stream_llm_response(model=model, messages=messages)\n",
    "# print(\"\\n\\n--------------------------------------------------------------------------------\\n\\n\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c292bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the system prompt for the extraction task\n",
    "\n",
    "# system_prompt = \"\"\"\n",
    "# You are an AI assistant that extracts **accurate, structured insights** from markdown articles.\n",
    "\n",
    "# ### Task\n",
    "# From the given markdown article, produce:\n",
    "\n",
    "# ## Key Takeaways\n",
    "# - Core conclusions, claims, or findings\n",
    "# - Important facts, numbers, decisions, or timelines\n",
    "\n",
    "# ## Summary\n",
    "# - A concise but complete overview\n",
    "# - Capture the main topic, purpose, and key points\n",
    "\n",
    "# ### Rules\n",
    "# - Preserve meaning, context, and important qualifiers\n",
    "# - Do not add opinions or external information\n",
    "# - Do not include meta commentary\n",
    "# - Use clear markdown headings and bullet points\n",
    "# - Be concise but information-dense\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "360a5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the system prompt for the extraction task\n",
    "\n",
    "# system_prompt = \"\"\"\n",
    "# You are an expert at extracting accurate, structured summaries and key takeaways from markdown articles.\n",
    "# You are given a web scraped markdown article. \n",
    "# Your task is to extract the key takeaways and a concise summary from the article.\n",
    "# The output should be in markdown format.\n",
    "# Reply only with the markdown content, do not include any explanations or commentary.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a0acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the user prompt for the extraction task (for scraped-content/main_content.md)\n",
    "\n",
    "# with open(\"scraped-content/main_content.md\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     content = f.read()\n",
    "\n",
    "# user_prompt = f\"\"\"\n",
    "# Here is a web scraped markdown article. \n",
    "# Extract the key takeaways and a concise summary according to the system prompt instructions.\n",
    "\n",
    "# Article:\n",
    "# {content}\n",
    "\n",
    "# Return the output in markdown format. \n",
    "# Do not include any explanations or commentary, reply only with the markdown content.\n",
    "# Do not explain your reasoning. \n",
    "# Start output immediately.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16dacf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c1127c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # call the model to perform the extraction task\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": system_prompt},\n",
    "#     {\"role\": \"user\", \"content\": user_prompt}\n",
    "# ]\n",
    "\n",
    "# model = \"llama3.1\"\n",
    "\n",
    "# response = stream_llm_response(model=model, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1843be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a quantization config for loading the model in 4-bit precision using bitsandbytes\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8312dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to stream response from the model, update the display in real-time, and return the final response as a string\n",
    "\n",
    "# simple in-memory cache to avoid reloading model every call\n",
    "_cache = {}\n",
    "\n",
    "def stream_response(model_id, messages):\n",
    "    # load tokenizer + model once per model_id\n",
    "    if model_id not in _cache:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "        model.eval()  # inference mode\n",
    "\n",
    "        _cache[model_id] = (tokenizer, model)\n",
    "\n",
    "    tokenizer, model = _cache[model_id]\n",
    "\n",
    "    # convert chat messages to model input tokens\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        return_tensors=\"pt\",\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # stream tokens as they are generated\n",
    "    streamer = TextStreamer(\n",
    "        tokenizer,\n",
    "        skip_prompt=True,\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # generate response\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1000\n",
    "    )\n",
    "\n",
    "    # remove prompt tokens and keep only generated output\n",
    "    generated_tokens = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "    # return final assistant output as string\n",
    "    return tokenizer.decode(\n",
    "        generated_tokens,\n",
    "        skip_special_tokens=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43cb329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model_id and messages for testing the streaming function\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Tell a joke for a room of Data Scientists\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be0a3f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 146/146 [00:00<00:00, 433.99it/s, Materializing param=model.norm.weight]                              \n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a joke tailored for a room of data scientists:\n",
      "\n",
      "Why did the data set go to therapy?\n",
      "\n",
      "Because it had a lot of 'data' to process, and it was struggling to 'handle' its emotions. But in the end, it just needed to 'analyze' its feelings and work through its issues.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Here's a joke tailored for a room of data scientists:\n",
      "\n",
      "Why did the data set go to therapy?\n",
      "\n",
      "Because it had a lot of 'data' to process, and it was struggling to 'handle' its emotions. But in the end, it just needed to 'analyze' its feelings and work through its issues.\n"
     ]
    }
   ],
   "source": [
    "# call the streaming function and print the final response\n",
    "\n",
    "response = stream_response(model_id=model_id, messages=messages)\n",
    "print(\"\\n\\n--------------------------------------------------------------------------------\\n\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3e1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "article-explorer (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
